{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cup detection\n",
    "\n",
    "### To build and to tune a simple detection model I will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils as imt\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "I have prepared dataset from an example video. I cut frames from this video by using ffmpeg. After that, I marked cups by using labelImg application (https://github.com/tzutalin/labelImg). I did it to tune a detection model.\n",
    "\n",
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"training/data/cut_video\"\n",
    "\n",
    "def xml_to_df(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see one of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_row(row):\n",
    "    img = cv2.imread(ROOT_PATH +\"/\" + row.filename, 1)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    lt_point = (row.xmin, row.ymin)\n",
    "    rb_point = (row.xmax, row.ymax)\n",
    "    COLOR = (0,255,0)\n",
    "    cv2.rectangle(img, lt_point, rb_point, COLOR, 4)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xml_to_df(ROOT_PATH + \"/annotations\")\n",
    "show_row(df.iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple HSV treshhold model.\n",
    "\n",
    "I will use information about a cup color. It seems to me we can find this cup in the HSV color model by finding a color interval. To do that I apply a symmetric couple of the threshold function for each color dimensions. After that, I can apply logic operation AND between the color dimension. The result matrix I will call a color mask matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_holding(rgb, color, weight):\n",
    "    img = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "    thresh_lambda = lambda ind: cv2.bitwise_and(\\\n",
    "                cv2.threshold(img[:,:,ind],\\\n",
    "                color[ind]-weight[ind],255,\\\n",
    "                cv2.THRESH_BINARY)[1],\\\n",
    "                cv2.threshold(img[:,:,ind],\\\n",
    "                color[ind]+weight[ind],255,\\\n",
    "                cv2.THRESH_BINARY_INV)[1],\\\n",
    "                )\n",
    "\n",
    "    thrs = [thresh_lambda(i) for i in range(3)]\n",
    "    rgthr = cv2.bitwise_and(thrs[0], thrs[1])\n",
    "    bthr = cv2.bitwise_and(thrs[2], rgthr)\n",
    "    return bthr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the color mask matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(ROOT_PATH +\"/\" + df.iloc[3].filename,1)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = thresh_holding(img, (125,70,70), (20,40,40))\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prehandling\n",
    "\n",
    "I apply adaptive histogram equalization for input picture. It a little bit allows avoiding the problem with different brightness and contrasts. OpenCV has CLAHE for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prehandling(bgr, clipLimit=20.0, tileGridSize=(6,6)):\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "    lab_planes = cv2.split(lab)\n",
    "    lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "    lab = cv2.merge(lab_planes)\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(ROOT_PATH +\"/\" + df.iloc[3].filename,1)\n",
    "img = prehandling(img, clipLimit=5)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the effect of prehandling to color mask matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(ROOT_PATH +\"/\" + df.iloc[3].filename,1)\n",
    "img = prehandling(img, clipLimit=5, tileGridSize=(6,6))\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = thresh_holding(img, (125,60,70), (20,40,40))\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting process\n",
    "\n",
    "### Area overlap calculate function\n",
    "\n",
    "To avoid the same bounding boxes I have to estimate the area overlap of this boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(rect1, rect2):\n",
    "    \"\"\"\n",
    "    calculate area overlap\n",
    "    rect is two point left top and bottom right\n",
    "    \"\"\"\n",
    "    delta_x = min(rect2[1][0], rect1[1][0]) - max(rect2[0][0], rect1[0][0])\n",
    "    delta_y = min(rect2[1][1], rect1[1][1]) - max(rect2[0][1], rect1[0][1])\n",
    "    common = delta_x * delta_y if delta_x > 0 and delta_y > 0 else 0\n",
    "    area1 = abs(rect1[0][0] - rect1[1][0]) * abs(rect1[0][1] - rect1[1][1])\n",
    "    area2 = abs(rect2[0][0] - rect2[1][0]) * abs(rect2[0][1] - rect2[1][1])\n",
    "    return 2 * common / (area1 + area2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection function\n",
    "\n",
    "To find bounding boxes I will use a matchTemplate OpenCV function. I will fill a template with max_value. After that, I will apply it to color_mask_matrix. To avoid duplicates I will union rectangles with a big overlap area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_fill_calc(color_mask_matrix,  window_size=250, threshhold=0.5, overlap_threshhold=0.75):\n",
    "    \n",
    "    template = np.full((window_size, window_size), 255, dtype=gray_scale.dtype)\n",
    "    \n",
    "    matchResult = cv2.matchTemplate(color_mask_matrix, template, cv2.TM_CCORR_NORMED)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(matchResult)\n",
    "    \n",
    "    #handmade normalization\n",
    "    res = matchResult / max_val \n",
    "    \n",
    "    #plt.imshow(res, cmap=\"gray\")\n",
    "    results = []\n",
    "    min_val, max_val, min_loc, max_loc = 0, 1, 0, 0\n",
    "    while max_val > threshhold:\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "        if max_val > threshhold:\n",
    "            bottom_right = (max_loc[0]+window_size, max_loc[1]+window_size)\n",
    "            results.append((max_loc, bottom_right))\n",
    "            \n",
    "            #to avoid finding this place the next time\n",
    "            res[max_loc[1]:bottom_right[1], max_loc[0]:bottom_right[0]] = 0\n",
    "    \n",
    "    \n",
    "    #union rectangles with a big overlap area\n",
    "    i = 0\n",
    "    while i < len(results):\n",
    "        detect = results[i]\n",
    "        \n",
    "        j = 0\n",
    "        while j < len(results):\n",
    "            if j != i and overlap(detect, results[j]) > overlap_threshhold:\n",
    "                #union two rect\n",
    "                xl, xr = min(detect[0][0], results[j][0][0]), max(detect[1][0], results[j][1][0])\n",
    "                yl, yr = min(detect[0][1], results[j][0][1]), max(detect[1][1], results[j][1][1])\n",
    "                results[i] = ((xl, yl), (xr, yr))\n",
    "                detect = results[i]\n",
    "                \n",
    "                \n",
    "                results.remove(results[j])\n",
    "                \n",
    "                if j < i:\n",
    "                    i -= 1\n",
    "            j += 1\n",
    "        \n",
    "        i += 1\n",
    "                \n",
    "                \n",
    "    \n",
    "    return results\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the example of work with no tuned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(img, detectins):\n",
    "    COLOR = (255, 0, 0)\n",
    "    for detected in detections:\n",
    "        cv2.rectangle(img, detected[0],  detected[1], COLOR, 3)\n",
    "\n",
    "img_source = cv2.imread(ROOT_PATH +\"/\" + df.iloc[9].filename,1)\n",
    "img = prehandling(img_source, clipLimit=5, tileGridSize=(6,6))\n",
    "img_pre = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = thresh_holding(img_pre, (125,60,70), (20,40,40))\n",
    "detections = windows_fill_calc(img, threshhold=0.98, window_size=200)\n",
    "draw_detections(img_pre, detections)\n",
    "plt.imshow(img_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire model\n",
    "\n",
    "Let compose model with these steps. I mean I will build detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHSVDetector:\n",
    "    \n",
    "    def __init__(self, color=(125,70,70), weight_color=(20,40,40),  window_size=250, \n",
    "                 threshhold=0.98, overlap_threshhold=0.75,\n",
    "                 clipLimit=5.0, tileGridSize=(6,6)):\n",
    "        \"\"\"\n",
    "        color should in HSV\n",
    "        \"\"\"\n",
    "        self.color, self.weight_color, self.window_size = color, weight_color, window_size\n",
    "        self.threshhold, self.overlap_threshhold = threshhold, overlap_threshhold\n",
    "        self.clipLimit, self.tileGridSize = clipLimit, tileGridSize\n",
    "        \n",
    "    def detect(self, bgr):\n",
    "        bgr = prehandling(bgr, clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "        rgb = cv2.cvtColor(bgr,cv2.COLOR_BGR2RGB)\n",
    "        gray_scale = thresh_holding(rgb, self.color, self.weight_color)\n",
    "        \n",
    "        return windows_fill_calc(img, threshhold=self.threshhold, window_size=self.overlap_threshhold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate process.\n",
    "\n",
    "Here I want to calculate error of detection model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Estimate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model parametrs.\n",
    "\n",
    "We can improve the quality of the model by changing the next parameters:\n",
    "\n",
    "* target color\n",
    "* diaposon for target color\n",
    "* threshholds\n",
    "* window size\n",
    "* clahe parametrs\n",
    "\n",
    "To find the best parameters we can create a grid search of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
